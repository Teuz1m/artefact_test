#  AI Assistant Vaga AI Engineer JÃºnior

Assistente de IA inteligente que identifica automaticamente quando usar ferramentas externas (calculadora) versus conhecimento base do LLM.

##  Objetivo

Criar um assistente que:
- Identifica perguntas matemÃ¡ticas automaticamente
- Usa calculadora para cÃ¡lculos precisos
- Responde com conhecimento base do LLM para outras perguntas
- Demonstra integraÃ§Ã£o de LLMs com ferramentas externas

##  Arquitetura

A arquitetura segue um modelo modular e desacoplado:

1. Frontend (Streamlit â€” porta 8501)

Interface interativa responsÃ¡vel por capturar inputs e exibir as respostas.

2. Backend (FastAPI â€” porta 8000)

Gateway de comunicaÃ§Ã£o entre o usuÃ¡rio e o agente.

3. MCP Server (FastMCP â€” porta 8001)

Servidor que expÃµe ferramentas determinÃ­sticas ao agente LLM:

Calculator: operaÃ§Ãµes matemÃ¡ticas seguras

Get_weather: consulta de clima em tempo real via OpenWeatherMap

**Fluxo Geral**: **Streamlit (Input do UsuÃ¡rio) â†’ FastAPI â†’ LangChain Agent(Escolha se necessita ou nao de ferramentas) â†’ MCP Tools â†’ Resposta ao usuÃ¡rio**

# Componentes

1. **MCP Server (FastMCP)**: Servidor de ferramentas
2. **Backend FastAPI**: API REST com agente LangChain
3. **Frontend Streamlit**: Interface web

##Como Executar


### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**

2. **Crie ambiente virtual e instale dependÃªncias**:

```bash
python3 -m venv venv
source venv/bin/activate

pip install --upgrade pip
pip install -r requirements.txt
```

3. **Configure as variÃ¡veis de ambiente**:

```bash
cp .env.example .env
```

Edite o arquivo `.env` e adicione suas chaves de API:

```env
# ObrigatÃ³rio: OpenAI API - necessita de uma API KEY da OPENAI com Creditos na conta
OPENAI_API_KEY=sk-sua-chave-aqui
OPENAI_MODEL=gpt-4

#Weather API (para usar a tool get_weather)
# Obtenha grÃ¡tis em: https://openweathermap.org/api
OPENWEATHER_API_KEY=sua-chave-openweather-aqui
```

###  Executar com Docker (Recomendado)

A forma mais fÃ¡cil de subir a aplicaÃ§Ã£o Ã© usando Docker Compose:

```bash
docker-compose up --build
```

Isso irÃ¡:
1. Construir as imagens Docker
2. Iniciar MCP Server (porta 8001)
3. Iniciar Backend FastAPI (porta 8000)
4. Iniciar Frontend Streamlit (porta 8501)

**Acesse `http://localhost:8501` para usar o serviÃ§o por Completo**

**Parar os serviÃ§os:**
```bash
docker-compose down
```



Executar cada serviÃ§o **separadamente**

**Terminal 1 - Backend FastAPI**:
```bash
python -m backend.main
```

O backend estarÃ¡ disponÃ­vel em `http://localhost:8000`

**Terminal 2 - Frontend Streamlit**:
```bash
streamlit run frontend/app.py
```

O frontend estarÃ¡ disponÃ­vel em `http://localhost:8501`

**Terminal 3 - Servidores MCP**:
```bash
python mcp_server.server.py
```

```

VocÃª pode testar a API sem o frontend, mas tem de executar Backend Fast API e Servidores MCP:

```bash
curl -X POST "http://localhost:8000/v1/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "Quanto Ã© 25 * 37?"}'

```
### Voce pode acessar documentaÃ§Ã£o interativa: `http://localhost:8000/docs`
```
/v1/query - Processa query de entrada do usuÃ¡rio
/v1/health - Health check da API
/v1/metric - Retorna mÃ©tricas de uso do sistema
```

**Os logs da API estÃ£o disponiveis na pasta 'logs'**


##  LÃ³gica de ImplementaÃ§Ã£o
A lÃ³gica utilizada foi ter uma API criada com FastAPI para ter um agente LLM que decidisse qual tools MCP usar baseada na entrada do usuÃ¡rio, para isso criei um MCP Server que contÃ©m duas ferramentas : calculator - uma calculado em python e get_wheater - conexao com uma API externa.

Foi dockerizado para estar sempre pronto para ser executado em diferentes servless.

A intenÃ§Ã£o era deixar a aplicaÃ§Ã£o o mais modular possivel, como um microsserviÃ§o.

### 1. MCP Server com FastMCP

Implementei um servidor MCP usando FastMCP que expÃµe 2 ferramentas:

**Tool 1: Calculator**
- **SeguranÃ§a**: Usa AST (Abstract Syntax Tree) para parse seguro.
- **OperaÃ§Ãµes suportadas**: `+`, `-`, `*`, `/`, `**` (potÃªncia), parÃªnteses


**Tool 2: Weather (get_weather)**
- **API**: OpenWeatherMap (gratuita, 60 calls/min)
- **ParÃ¢metros**: city (string), country_code (default: "BR")
- **Retorna**: Temperatura, descriÃ§Ã£o, umidade, velocidade do vento


### 2. LangChain Agent

O agente usa OpenAI Functions para decidir automaticamente quando usar a calculadora ou a consulta do clima:

- **System Prompt**: InstruÃ§Ãµes claras sobre quando usar cada ferramenta
- **OpenAI Functions Agent**: Framework nativo do LangChain para function calling
- **IntegraÃ§Ã£o com MCP**: Conecta o agente Ã s ferramentas do MCP Server


### 3. Backend FastAPI

API REST que expÃµe endpoint `/v1/query`:

- **POST /v1/query**: Processa pergunta do usuÃ¡rio
- **GET /v1/health**: Health check
- **CORS habilitado**: Para comunicaÃ§Ã£o com frontend Streamlit

### 4. Frontend Streamlit

Interface web simples e intuitiva:

- Chat interativo
- VisualizaÃ§Ã£o de tools usadas
- Exemplos sugeridos de perguntas

##  DecisÃµes TÃ©cnicas

### Por que FastMCP?

- **Simplicidade**: Reduz boilerplate comparado com implementaÃ§Ã£o manual
- **PadronizaÃ§Ã£o**: Segue o Model Context Protocol para conexÃµes com LLM
- **IntegraÃ§Ã£o fÃ¡cil**: Funciona nativamente com LangChain
- **Desacoplamento**: Pode ser usado em um conteiner separadamente para ser usado por outros agentes LLM

### Por que LangChain?

- **Agents prontos**: Framework maduro para agentes com ferramentas
- **OpenAI Functions**: Suporte nativo a function calling

### Por que FastAPI?

- **Async nativo**: Suporta operaÃ§Ãµes assÃ­ncronas do LangChain
- **Type hints**: ValidaÃ§Ã£o automÃ¡tica com Pydantic
- **DocumentaÃ§Ã£o automÃ¡tica**: Swagger UI

### Por que Streamlit?

- **Desenvolvimento rÃ¡pido**
- **Ideal para demonstraÃ§Ãµes**

##  Melhorias Futuras

Se tivesse mais tempo, implementaria:

### Funcionalidades

- **Mais ferramentas**: Busca web, consulta mais APIs
- **RAG**: Base de conhecimento para busca semantica
- **Streaming**: Respostas em tempo real via WebSockets, permitindo menor latÃªncia
- **HistÃ³rico**: PersistÃªncia de conversas a longo prazo em banco de dados
- **Multi-agente**: CoordenaÃ§Ã£o entre mÃºltiplos agentes especializados

### Infraestrutura
- **ModularizaÃ§Ã£o**: Cada MCP Server com seu prÃ³pio conteiner, permitindo ser usado por outros Agentes LLM
- **Rate Limiting**: ProteÃ§Ã£o contra abuso
- **AutenticaÃ§Ã£o**: JWT ou API Keys
- **Observabilidade**: LangSmith para tracing de monitoramento
- **Mensageria**: Usar RabbitMQ para ter um sistema de Filas na conexao LLM <-> MCP Server 

### Qualidade

- **CI/CD**: GitHub Actions para testes e deploy automÃ¡ticos
- **Linting**


## ğŸ§ª Testes


### Rodar todos os testes

Deixa o serviÃ§o MCP online :
  ```bash
    cd mcp_server
    python ./server.py
  ```

```bash
pytest -v
```

## Estrutura do Projeto

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ mcp_server/
â”‚   â””â”€â”€ server.py
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ agent.py
â”‚       â””â”€â”€ prompts.py
â”‚
â””â”€â”€ frontend/
    â””â”€â”€ app.py
```
## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como parte de um desafio tÃ©cnico para vaga de AI Engineer JÃºnior.

---
